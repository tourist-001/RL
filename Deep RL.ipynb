{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.atari as atari\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment BreakoutDeterministic-v0 is out of date. You should consider upgrading to version `v4` with the environment ID `BreakoutDeterministic-v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"Breakout\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Breakout\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Breakout\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1ddf7d258455>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BreakoutDeterministic-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;31m# fmt: on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Env\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;31m# Construct the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# Make the environment aware of which spec it came from.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, render_mode)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m# Seed + Load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         self._action_set = (\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\environment.py\u001b[0m in \u001b[0;36mseed\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             raise error.Error(\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;34mf'We\\'re Unable to find the game \"{self._game}\". Note: Gym no longer distributes ROMs. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[1;34mf\"If you own a license to use the necessary ROMs for research purposes you can download them \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: We're Unable to find the game \"Breakout\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Breakout\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Breakout\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3f2152c088ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.observation_space.shape)\n",
    "print(np.min(env.observation_space.low))\n",
    "print(np.max(env.observation_space.high))\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = env.reset()\n",
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(1)\n",
    "env.render()\n",
    "for i in range(150):\n",
    "    a = 2+np.random.randint(2)\n",
    "    env.step(a)\n",
    "    env.render()\n",
    "env.step(1)\n",
    "for i in range(150):\n",
    "    a = 2+np.random.randint(2)\n",
    "    env.step(a)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 available actions in Breakout are as follows:\n",
    "- 0 NOOP (no operation)\n",
    "- 1 FIRE (press fire button)\n",
    "- 2 RIGHT (move paddle right)\n",
    "- 3 LEFT (move paddle left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "s, r, d, info = env.step(1)\n",
    "print(s.shape)\n",
    "print(r)\n",
    "print(d)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we press a button, the `step` function returns:\n",
    "- the next screen, \n",
    "- a \"reward\" signal indicating how much we've won during this time step,\n",
    "- a boolean indicating if we've lost,\n",
    "- some extra information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural networks and Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "def process_screen(x):\n",
    "    return 256*resize(rgb2gray(x), (110,84))[17:101,:]\n",
    "\n",
    "y=process_screen(x)\n",
    "plt.imshow(y, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the 4 last frames\n",
    "z = np.stack([y,y,y,y],axis=-1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "dqn = Sequential()\n",
    "#1st layer\n",
    "dqn.add(Conv2D(filters=16, kernel_size=(8,8), strides=4, activation=\"relu\", input_shape=(84,84,4)))\n",
    "#2nd layer\n",
    "dqn.add(Conv2D(filters=32, kernel_size=(4,4), strides=2, activation=\"relu\"))\n",
    "dqn.add(Flatten())\n",
    "#3rd layer\n",
    "dqn.add(Dense(units=256, activation=\"relu\"))\n",
    "#output layer\n",
    "dqn.add(Dense(units=4, activation=\"linear\"))\n",
    "\n",
    "dqn.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(dqn, to_file=\"images/dqn_keras.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 10000000\n",
    "replay_memory_size = 100000\n",
    "mini_batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "def epsilon(step):\n",
    "    if step<1e6:\n",
    "        return 1.-step*9e-7\n",
    "    return .1\n",
    "\n",
    "def clip_reward(r):\n",
    "    rr=0\n",
    "    if r>0:\n",
    "        rr=1\n",
    "    if r<0:\n",
    "        rr=-1\n",
    "    return rr\n",
    "\n",
    "def greedy_action(network, x):\n",
    "    Q = network.predict(np.array([x]))\n",
    "    return np.argmax(Q)\n",
    "\n",
    "def MCeval(network, trials, length, gamma):\n",
    "    scores = np.zeros((trials))\n",
    "    for i in range(trials):\n",
    "        screen_x = process_screen(env.reset())\n",
    "        stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "        x = np.stack(stacked_x, axis=-1)\n",
    "        for t in range(length):\n",
    "            a = greedy_action(network, x)\n",
    "            raw_screen_y, r, d, _ = env.step(a)\n",
    "            r = clip_reward(r)\n",
    "            screen_y = process_screen(raw_screen_y)\n",
    "            scores[i] = scores[i] + gamma**t * r\n",
    "            if d==True:\n",
    "                # restart episode\n",
    "                screen_x = process_screen(env.reset())\n",
    "                stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "                x = np.stack(stacked_x, axis=-1)\n",
    "            else:\n",
    "                # keep going\n",
    "                screen_x = screen_y\n",
    "                stacked_x.append(screen_x)\n",
    "                x = np.stack(stacked_x, axis=-1)\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the replay memory\n",
    "from collections import deque\n",
    "\n",
    "class MemoryBuffer:\n",
    "    \"An experience replay buffer using numpy arrays\"\n",
    "    def __init__(self, length, screen_shape, action_shape):\n",
    "        self.length = length\n",
    "        self.screen_shape = screen_shape\n",
    "        self.action_shape = action_shape\n",
    "        shape = (length,) + screen_shape\n",
    "        self.screens_x = np.zeros(shape, dtype=np.uint8) # starting states\n",
    "        self.screens_y = np.zeros(shape, dtype=np.uint8) # resulting states\n",
    "        shape = (length,) + action_shape\n",
    "        self.actions = np.zeros(shape, dtype=np.uint8) # actions\n",
    "        self.rewards = np.zeros((length,1), dtype=np.uint8) # rewards\n",
    "        self.terminals = np.zeros((length,1), dtype=np.bool) # true if resulting state is terminal\n",
    "        self.terminals[-1] = True\n",
    "        self.index = 0 # points one position past the last inserted element\n",
    "        self.size = 0 # current size of the buffer\n",
    "    \n",
    "    def append(self, screenx, a, r, screeny, d):\n",
    "        self.screens_x[self.index] = screenx\n",
    "        #plt.imshow(screenx)\n",
    "        #plt.show()\n",
    "        #plt.imshow(self.screens_x[self.index])\n",
    "        #plt.show()\n",
    "        self.actions[self.index] = a\n",
    "        self.rewards[self.index] = r\n",
    "        self.screens_y[self.index] = screeny\n",
    "        self.terminals[self.index] = d\n",
    "        self.index = (self.index+1) % self.length\n",
    "        self.size = np.min([self.size+1,self.length])\n",
    "    \n",
    "    def stacked_frames_x(self, index):\n",
    "        im_deque = deque(maxlen=4)\n",
    "        pos = index % self.length\n",
    "        for i in range(4): # todo\n",
    "            im = self.screens_x[pos]\n",
    "            im_deque.appendleft(im)\n",
    "            test_pos = (pos-1) % self.length\n",
    "            if self.terminals[test_pos] == False:\n",
    "                pos = test_pos\n",
    "        return np.stack(im_deque, axis=-1)\n",
    "    \n",
    "    def stacked_frames_y(self, index):\n",
    "        im_deque = deque(maxlen=4)\n",
    "        pos = index % self.length\n",
    "        for i in range(4): # todo\n",
    "            im = self.screens_y[pos]\n",
    "            im_deque.appendleft(im)\n",
    "            test_pos = (pos-1) % self.length\n",
    "            if self.terminals[test_pos] == False:\n",
    "                pos = test_pos\n",
    "        return np.stack(im_deque, axis=-1)\n",
    "    \n",
    "    def minibatch(self, size):\n",
    "        #return np.random.choice(self.data[:self.size], size=sz, replace=False)\n",
    "        indices = np.random.choice(self.size, size=size, replace=False)\n",
    "        x = np.zeros((size,)+self.screen_shape+(4,))\n",
    "        y = np.zeros((size,)+self.screen_shape+(4,))\n",
    "        for i in range(size):\n",
    "            x[i] = self.stacked_frames_x(indices[i])\n",
    "            y[i] = self.stacked_frames_y(indices[i])\n",
    "        return x, self.actions[indices], self.rewards[indices], y, self.terminals[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize state and replay memory\n",
    "screen_x = process_screen(env.reset())\n",
    "stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "x = np.stack(stacked_x, axis=-1)\n",
    "replay_memory = MemoryBuffer(replay_memory_size, (84, 84), (1,))\n",
    "# initial state for evaluation\n",
    "evaluation_period = 10000\n",
    "Xtest = np.array([x])\n",
    "nb_epochs = total_steps // evaluation_period\n",
    "epoch=-1\n",
    "scoreQ = np.zeros((nb_epochs))\n",
    "scoreMC = np.zeros((nb_epochs))\n",
    "\n",
    "# Deep Q-learning with experience replay\n",
    "for step in range(total_steps):\n",
    "    # evaluation\n",
    "    if(step%10000 == 0):\n",
    "        epoch = epoch+1\n",
    "        # evaluation of initial state\n",
    "        scoreQ[epoch] = np.mean(dqn.predict(Xtest).max(1))\n",
    "        # roll-out evaluation\n",
    "        scoreMC[epoch] = MCeval(network=dqn, trials=20, length=700, gamma=gamma)\n",
    "    # action selection\n",
    "    if np.random.rand() < epsilon(step):\n",
    "        a = np.random.randint(env.action_space.n)\n",
    "    else:\n",
    "        a = greedy_action(dqn, x)\n",
    "    # step\n",
    "    raw_screen_y, r, d, _ = env.step(a)\n",
    "    r = clip_reward(r)\n",
    "    screen_y = process_screen(raw_screen_y)\n",
    "    replay_memory.append(screen_x, a, r, screen_y, d)\n",
    "    # train\n",
    "    if step>mini_batch_size:\n",
    "        X,A,R,Y,D = replay_memory.minibatch(mini_batch_size)\n",
    "        QY = dqn.predict(Y)\n",
    "        QYmax = QY.max(1).reshape((mini_batch_size,1))\n",
    "        update = R + gamma * (1-D) * QYmax\n",
    "        QX = dqn.predict(X)\n",
    "        QX[np.arange(mini_batch_size), A.ravel()] = update.ravel()\n",
    "        dqn.train_on_batch(x=X, y=QX)\n",
    "    # prepare next transition\n",
    "    if d==True:\n",
    "        # restart episode\n",
    "        screen_x = process_screen(env.reset())\n",
    "        stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "        x = np.stack(stacked_x, axis=-1)\n",
    "    else:\n",
    "        # keep going\n",
    "        screen_x = screen_y\n",
    "        stacked_x.append(screen_x)\n",
    "        x = np.stack(stacked_x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"TmPfTpjtdgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"frozenlake\"></a> Frozen lake\n",
    "is a really easy game called FrozenLake. It is a discrete environment, easy to solve. It is provided in order to help you play around with the different concepts of Q-learning without any value function approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.1.2-cp38-cp38-win_amd64.whl (8.4 MB)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FrozenLakeEnv' object has no attribute 'lastaction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e896e57249cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FrozenLake-v1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_gui\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render_gui\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py\u001b[0m in \u001b[0;36m_render_gui\u001b[1;34m(self, desc, mode)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# prepare images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[0mlast_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlastaction\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlastaction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[0melf_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melf_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         elf_scale = min(\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'lastaction'"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "_=env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FrozenLakeEnv in module gym.envs.toy_text.frozen_lake:\n",
      "\n",
      "class FrozenLakeEnv(gym.core.Env)\n",
      " |  FrozenLakeEnv(*args, **kwds)\n",
      " |  \n",
      " |  Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over\n",
      " |  the Frozen(F) lake. The agent may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
      " |  \n",
      " |  \n",
      " |  ### Action Space\n",
      " |  The agent takes a 1-element vector for actions.\n",
      " |  The action space is `(dir)`, where `dir` decides direction to move in which can be:\n",
      " |  \n",
      " |  - 0: LEFT\n",
      " |  - 1: DOWN\n",
      " |  - 2: RIGHT\n",
      " |  - 3: UP\n",
      " |  \n",
      " |  ### Observation Space\n",
      " |  The observation is a value representing the agent's current position as\n",
      " |  current_row * nrows + current_col (where both the row and col start at 0).\n",
      " |  For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
      " |  The number of possible observations is dependent on the size of the map.\n",
      " |  For example, the 4x4 map has 16 possible observations.\n",
      " |  \n",
      " |  ### Rewards\n",
      " |  \n",
      " |  Reward schedule:\n",
      " |  - Reach goal(G): +1\n",
      " |  - Reach hole(H): 0\n",
      " |  - Reach frozen(F): 0\n",
      " |  \n",
      " |  ### Arguments\n",
      " |  \n",
      " |  ```\n",
      " |  gym.make('FrozenLake-v1', desc=None,map_name=\"4x4\", is_slippery=True)\n",
      " |  ```\n",
      " |  \n",
      " |  `desc`: Used to specify custom map for frozen lake. For example,\n",
      " |  \n",
      " |      desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"].\n",
      " |  \n",
      " |  `map_name`: ID to use any of the preloaded maps.\n",
      " |  \n",
      " |      \"4x4\":[\n",
      " |          \"SFFF\",\n",
      " |          \"FHFH\",\n",
      " |          \"FFFH\",\n",
      " |          \"HFFG\"\n",
      " |          ]\n",
      " |  \n",
      " |      \"8x8\": [\n",
      " |          \"SFFFFFFF\",\n",
      " |          \"FFFFFFFF\",\n",
      " |          \"FFFHFFFF\",\n",
      " |          \"FFFFFHFF\",\n",
      " |          \"FFFHFFFF\",\n",
      " |          \"FHHFFFHF\",\n",
      " |          \"FHFFHFHF\",\n",
      " |          \"FFFHFFFG\",\n",
      " |      ]\n",
      " |  \n",
      " |  `is_slippery`: True/False. If True will move in intended direction with\n",
      " |  probability of 1/3 else will move in either perpendicular direction with\n",
      " |  equal probability of 1/3 in both directions.\n",
      " |  \n",
      " |      For example, if action is left and is_slippery is True, then:\n",
      " |      - P(move left)=1/3\n",
      " |      - P(move up)=1/3\n",
      " |      - P(move down)=1/3\n",
      " |  \n",
      " |  ### Version History\n",
      " |  * v1: Bug fixes to rewards\n",
      " |  * v0: Initial versions release (1.0.0)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FrozenLakeEnv\n",
      " |      gym.core.Env\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, desc=None, map_name='4x4', is_slippery=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      third-party environments may not support rendering at all.)\n",
      " |      By convention, if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render_modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render_modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  reset(self, *, seed: Union[int, NoneType] = None, return_info: bool = False, options: Union[dict, NoneType] = None)\n",
      " |      Resets the environment to an initial state and returns an initial\n",
      " |      observation.\n",
      " |      \n",
      " |      This method should also reset the environment's random number\n",
      " |      generator(s) if `seed` is an integer or if the environment has not\n",
      " |      yet initialized a random number generator. If the environment already\n",
      " |      has a random number generator and `reset` is called with `seed=None`,\n",
      " |      the RNG should not be reset.\n",
      " |      Moreover, `reset` should (in the typical use case) be called with an\n",
      " |      integer seed right after initialization and then never again.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |          info (optional dictionary): a dictionary containing extra information, this is only returned if return_info is set to true\n",
      " |  \n",
      " |  step(self, a)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, logging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  metadata = {'render_fps': 4, 'render_modes': ['human', 'ansi', 'rgb_ar...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  np_random\n",
      " |      Initializes the np_random field if not done already.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  __annotations__ = {'_np_random': 'RandomNumberGenerator | None', 'acti...\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[~ObsType, ~ActType],)\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "help(fl.FrozenLakeEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "16\n",
      "Reset:  0\n",
      "Step right: (0, 0.0, False, {'prob': 0.3333333333333333})\n",
      "Second step right: (1, 0.0, False, {'prob': 0.3333333333333333})\n",
      "Third step right: (5, 0.0, True, {'prob': 0.3333333333333333})\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.n)\n",
    "print(\"Reset: \", env.reset())\n",
    "print(\"Step right:\", env.step(fl.RIGHT))\n",
    "env.render()\n",
    "print(\"Second step right:\", env.step(fl.RIGHT))\n",
    "print(\"Third step right:\", env.step(fl.RIGHT))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four utility functions and a bit of display help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '←', 1: '↓', 2: '→', 3: '↑'}\n"
     ]
    }
   ],
   "source": [
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n",
    "\n",
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "print(actions)\n",
    "\n",
    "def greedyQpolicy(Q):\n",
    "    pi = np.zeros((env.observation_space.n),dtype=np.int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        pi[s] = np.argmax(Q[s,:])\n",
    "    return pi\n",
    "\n",
    "def print_policy(pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help you study the convergence of Q-learning, we provide an approximate value for $Q^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qstar = np.array([[ 0.068,  0.066,  0.066,  0.059],\n",
    " [ 0.039,  0.043,  0.04,   0.061],\n",
    " [ 0.074,  0.068,  0.072,  0.057],\n",
    " [ 0.039,  0.039,  0.033,  0.055],\n",
    " [ 0.091,  0.071,  0.064,  0.048],\n",
    " [ 0.,     0.,     0.,     0.   ],\n",
    " [ 0.112,  0.09,   0.112,  0.022],\n",
    " [ 0.,     0.,     0.,     0.   ],\n",
    " [ 0.071,  0.118,  0.102,  0.145],\n",
    " [ 0.157,  0.247,  0.204,  0.133],\n",
    " [ 0.299,  0.266,  0.225,  0.108],\n",
    " [ 0.,     0.,     0.,     0.   ],\n",
    " [ 0.,     0.,     0.,     0.   ],\n",
    " [ 0.188,  0.306,  0.38,   0.266],\n",
    " [ 0.395,  0.639,  0.615,  0.537],\n",
    " [ 0.,     0.,     0.,     0.   ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-16e55a8be70b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Q-learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# to track update frequencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's initialize the Q-function\n",
    "Qql = np.zeros((16,4))\n",
    "max_steps = 5000000\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "\n",
    "def epsilon_greedy(Q, s, epsilon):\n",
    "    a = np.argmax(Q[s,:])\n",
    "    if(np.random.rand()<=epsilon): # random action\n",
    "        aa = np.random.randint(env.action_space.n-1)\n",
    "        if aa==a:\n",
    "            a=env.action_space.n-1\n",
    "        else:\n",
    "            a=aa\n",
    "    return a\n",
    "\n",
    "# Q-learning\n",
    "count = np.zeros((env.observation_space.n,env.action_space.n)) # to track update frequencies\n",
    "epsilon = 1\n",
    "x = env.reset()\n",
    "for t in range(max_steps):\n",
    "    if((t+1)%1000000==0):\n",
    "        epsilon = epsilon/2\n",
    "    a = epsilon_greedy(Qql,x,epsilon)\n",
    "    y,r,d,_ = env.step(a)\n",
    "    Qql[x][a] = Qql[x][a] + alpha * (r+gamma*np.max(Qql[y][:])-Qql[x][a])\n",
    "    count[x][a] += 1\n",
    "    if d==True:\n",
    "        x = env.reset()\n",
    "    else:\n",
    "        x=y\n",
    "\n",
    "# Q-learning's final value function and policy\n",
    "\n",
    "print(\"Max error:\", np.max(np.abs(Qql-Qstar)))\n",
    "print(\"Final epsilon:\", epsilon)\n",
    "pi_ql = greedyQpolicy(Qql)\n",
    "print(\"Greedy Q-learning policy:\")\n",
    "print_policy(pi_ql)\n",
    "print(\"Difference between pi_sarsa and pi_star (recall that there are several optimal policies):\")\n",
    "print(pi_ql-pi_star)\n",
    "Qpi_ql, residuals = policy_Qeval_iter(pi_ql,1e-4,10000)\n",
    "print(\"Max difference in value between pi_sarsa and pi_star:\", np.max(np.abs(Qpi_ql-Qstar)))\n",
    "print(\"Min difference in value between pi_sarsa and pi_star:\", np.min(np.abs(Qpi_ql-Qstar)))\n",
    "\n",
    "# Plot visitation frequencies map\n",
    "\n",
    "count_map = np.zeros((env.unwrapped.nrow, env.unwrapped.ncol, env.action_space.n))\n",
    "for a in range(env.action_space.n):\n",
    "    for x in range(env.observation_space.n):\n",
    "        row,col = to_row_col(x)\n",
    "        count_map[row, col, a] = count[x,a]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4)\n",
    "for a in range(env.action_space.n):\n",
    "    name = \"a = \" + actions[a]\n",
    "    axs[a].set_title(name)\n",
    "    axs[a].imshow(np.log(count_map[:,:,a]+1), interpolation='nearest')\n",
    "    #print(\"a=\", a, \":\", sep='')\n",
    "    #print(count_map[:,:,a])\n",
    "plt.show()\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"tictactoe\"></a> Deep Tic-Tac-Toe learning\n",
    "\n",
    "Let's implement the DQN algorithm on a simple Tic-Tac-Toe game.\n",
    "\n",
    "Note that this is not a Stochastic Optimal Control problem *per se*: it is an adversarial, two-player game (while stochastic optimal control is limited to one-player games). But maybe you can imagine a way of adapting Q-learning to this setting (think about how AlphaGo found better-than-human strategies at Go).\n",
    "\n",
    "This exercice is thus a way to push the boundaries of what we have seen before.\n",
    "\n",
    "###  The board game\n",
    "\n",
    "This is a quite simple implementation. The board is a tuple of size 9 where each action refers to a position in the tuple. We store the status of the current player and who won the game.\n",
    "\n",
    "The main classes and objects:\n",
    "\n",
    "* player(state) \n",
    "* available_move(state)\n",
    "* next_state(state, action, current_player)\n",
    "* win(board, player)\n",
    "* payoff(current_player, state) \n",
    "* play(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oxo:\n",
    "    def __init__(self):\n",
    "        self.current_player = 1\n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        self.board = (0,0,0,0,0,0,0,0,0)\n",
    "        self.nb_move = 0\n",
    "        self.end_game = 0 # -1: null, O: running, 1: player 1 win, 2: player 2 win\n",
    "\n",
    "    def play(self, state, action):\n",
    "        self.current_player = self.player(state)\n",
    "        stateList = list(self.board)\n",
    "        stateList[action] = self.current_player\n",
    "        self.board = tuple(stateList)\n",
    "        self.nb_move += 1\n",
    "        #print(\"nb move: \", self.nb_move)\n",
    "        self.actions.remove(action)\n",
    "        \n",
    "        if self.asWin():\n",
    "            self.end_game = self.current_player\n",
    "        if self.nb_move == 9 and self.end_game == 0:\n",
    "            self.end_game = -1\n",
    "\n",
    "    def next_state(self, state, action, player):\n",
    "        stateList = list(state)\n",
    "        stateList[action] = player\n",
    "        return tuple(stateList)\n",
    "\n",
    "    def available_move(self, state):\n",
    "        am = []\n",
    "        i = 0;\n",
    "        for x in state:\n",
    "            if x == 0: \n",
    "                am += [i]\n",
    "            i += 1\n",
    "        return am\n",
    "\n",
    "    def asWin(self):\n",
    "        p = self.current_player\n",
    "        b = self.board\n",
    "        if (b[0] == b[1] == b[2] == p or b[3] == b[4] == b[5] == p or b[6] == b[7] == b[8] == p or\n",
    "            b[0] == b[3] == b[6] == p or b[1] == b[4] == b[7] == p or b[2] == b[5] == b[8] == p or\n",
    "            b[0] == b[4] == b[8] == p or b[2] == b[4] == b[6] == p):\n",
    "            return True\n",
    "        else: return False\n",
    "\n",
    "    def win(self, b, p):\n",
    "        if (b[0] == b[1] == b[2] == p or b[3] == b[4] == b[5] == p or b[6] == b[7] == b[8] == p or\n",
    "            b[0] == b[3] == b[6] == p or b[1] == b[4] == b[7] == p or b[2] == b[5] == b[8] == p or\n",
    "            b[0] == b[4] == b[8] == p or b[2] == b[4] == b[6] == p):\n",
    "            return True\n",
    "        else: return False\n",
    "        \n",
    "\n",
    "    # -1: running, 0; exaecquo, 1 player 1, 2 player 2    \n",
    "    def payoff(self, p, b):\n",
    "        nb_move = 0\n",
    "        for i in b:\n",
    "            if i != 0: nb_move += 1\n",
    "\n",
    "        if self.win(b, 1):    return 1\n",
    "        if self.win(b, 2):    return 2    \n",
    "        if nb_move == 9: return 0        \n",
    "        return -1\n",
    "\n",
    "    def player(self, board):\n",
    "        J1=0\n",
    "        J2=0\n",
    "        for i in board:\n",
    "            if i==1: J1+=1 \n",
    "            if i==2: J2+=1 \n",
    "        if J1==J2: return 1\n",
    "        return(2)\n",
    "\n",
    "    def simulation(self):\n",
    "        while self.end_game == 0:\n",
    "            self.myPrint()\n",
    "            print (\"actions \", self.actions)\n",
    "            action = int(input(\"Player %s: \" % (self.current_player)))\n",
    "\n",
    "            if action in self.actions: self.play(self.board, action)\n",
    "            else: print (\"wrong move, try again\")\n",
    "\n",
    "            if self.asWin(): \n",
    "                print(\"Player \" , self.current_player , \" win!!!\")\n",
    "                self.end_game = self.current_player\n",
    "\n",
    "            if self.nb_move == 9 and self.end_game == 0: \n",
    "                print(\"No winner, No looser!\")\n",
    "                self.end_game == -1\n",
    "\n",
    "            if self.current_player == 1: self.current_player = 2\n",
    "            else: self.current_player = 1\n",
    "\n",
    "    def myPrint(self):\n",
    "        b = []\n",
    "        for x in self.board:\n",
    "            if x == 1: b.append('X')\n",
    "            else: \n",
    "                if x == 2: b.append('O')\n",
    "                else: b.append('.')\n",
    "        print()\n",
    "        print(\"     \", b[0] , \"  \" , b[1] , \"  \" , b[2], \"       \", 0 , \"  \" , 1 , \"  \" , 2)\n",
    "        print(\"     \", b[3] , \"  \" , b[4] , \"  \" , b[5], \"  ->   \", 3 , \"  \" , 4 , \"  \" , 5)\n",
    "        print(\"     \", b[6] , \"  \" , b[7] , \"  \" , b[8], \"       \", 6 , \"  \" , 7 , \"  \" , 8)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you implement an efficient DQN player?\n",
    "\n",
    "We provide a test function below to let you play against your deep Q learning agent. It supposes `model` is a keras-style deep neural network. The second argument states if the AI plays first (`playerAI=1`) or second (`playerAI=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAlgo(model, playerAI):\n",
    "    loop = True\n",
    "    while (loop):\n",
    "        jeu = oxo()\n",
    "        #switch first player at each round\n",
    "        if playerAI==1:\n",
    "            playerAI=2\n",
    "        else:\n",
    "            playerAI=1\n",
    "        #while game still in progress\n",
    "        while(jeu.end_game==0):\n",
    "            state = jeu.board\n",
    "            current_player = jeu.player(state)\n",
    "            b = current_player\n",
    "            if b == 1: b='X'\n",
    "            else: b = 'O'\n",
    "            \n",
    "            if current_player == playerAI: \n",
    "                \n",
    "                qval = model.predict(np.array(state).reshape(1, len(state)), batch_size=batchSize)\n",
    "                print(\"State=\", state)\n",
    "                print(\"Actions:\", jeu.actions)\n",
    "                for i in range(len(qval[0])):\n",
    "                    print(\"     Action:\", i, \"Q-value:\", qval[0][i])\n",
    "                qval_av_action = [-9999]*9\n",
    "                for ac in jeu.actions:\n",
    "                    qval_av_action[ac] = qval[0][ac]\n",
    "                #print(qval_av_action)\n",
    "                action = (np.argmax(qval_av_action))\n",
    "                print(\"Action:\", action)\n",
    "            else:\n",
    "                jeu.myPrint()\n",
    "                print (\"action \", jeu.actions, \" current_player = \", b)\n",
    "                action = int(input(\"Player %s: \" % (current_player)))\n",
    "        \n",
    "            if action == 10:\n",
    "                loop=False\n",
    "                break \n",
    "            \n",
    "            if action in jeu.actions: \n",
    "                jeu.play(state, action)\n",
    "            else: \n",
    "                print (\"----- > Wrong move, try again!\")\n",
    "                \n",
    "            if jeu.asWin():\n",
    "                if current_player == playerAI:\n",
    "                    print(\"-------------------------------\")\n",
    "                    print(\"--------->  AI WINS <----------\")\n",
    "                    print(\"-------------------------------\")\n",
    "                else:\n",
    "                    print(\"-------------------------------\")\n",
    "                    print(\"----------> YOU WIN <----------\")\n",
    "                    print(\"-------------------------------\")\n",
    "             \n",
    "            if jeu.nb_move == 9 and jeu.end_game == 0:\n",
    "                print(\"-------------------------------\")\n",
    "                print(\"---> No winner, No looser <----\")\n",
    "                print(\"-------------------------------\")\n",
    "            \n",
    "            #clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RMSprop' from 'keras.optimizers' (C:\\Users\\Yu rui\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-38df2375e9d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RMSprop' from 'keras.optimizers' (C:\\Users\\Yu rui\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\optimizers.py)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, sgd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# The Deep Q network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(150, init='lecun_uniform', input_shape=(9,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(150, init='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(9, init='lecun_uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss='mse', optimizer=\"rmsprop\")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Parameters for Q-learning\n",
    "\n",
    "epochs = 1000\n",
    "gamma = 0.99 # discount factor\n",
    "epsilon = 1 # epsilon-greddy\n",
    "\n",
    "update=0 \n",
    "alpha=0.1 # learning rate\n",
    "experience_replay=True\n",
    "batchSize = 40 # mini batch size\n",
    "buffer = 1000\n",
    "replay = [] # init vector buffer\n",
    "h=0 # current size of the vector buffer\n",
    "\n",
    "# Initialize game\n",
    "\n",
    "jeu = oxo()\n",
    "\n",
    "# Learn\n",
    "\n",
    "for i in range(epochs):\n",
    "    jeu = oxo()\n",
    "    state = jeu.board\n",
    "    current_player = jeu.player(state)\n",
    "    \n",
    "    while(jeu.end_game==0):\n",
    "        current_player = jeu.player(state)\n",
    "        qval = model.predict(np.array(state).reshape(1,len(state)), batch_size=batchSize)\n",
    "        if (random.random() < epsilon): # exploration exploitation strategy    \n",
    "            rand = np.random.randint(0,len(jeu.actions))\n",
    "            action = jeu.actions[rand]\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            qval_av_action = [-9999]*9\n",
    "            for ac in jeu.actions:\n",
    "                qval_av_action[ac] = qval[0][ac]\n",
    "            action = (np.argmax(qval_av_action))\n",
    "        #Take action, observe new state S'\n",
    "        #Observe reward\n",
    "        jeu.play(state, action)\n",
    "        new_state = jeu.board\n",
    "        # choose new reward values\n",
    "        reward = -5\n",
    "        if jeu.payoff(current_player, new_state) == current_player:\n",
    "            reward = 2\n",
    "        if jeu.payoff(current_player, new_state) == 0:\n",
    "            reward = 1\n",
    "        if jeu.payoff(current_player, new_state) == -1:\n",
    "            reward = 0\n",
    "            \n",
    "        if not experience_replay:\n",
    "            #Get max_Q(S',a)\n",
    "            newQ = model.predict(np.array(new_state).reshape(1,len(state)), batch_size=batchSize)\n",
    "            maxQ = np.max(newQ)\n",
    "            y = np.zeros((1,9))\n",
    "            y[:] = qval[:]\n",
    "            if reward != 0: #non-terminal state\n",
    "                update = (reward + gamma * maxQ)\n",
    "            else:\n",
    "                update = reward\n",
    "            y[0][action] = update #target output\n",
    "            print(\"Game #: %s\" % (i,))\n",
    "            model.fit(np.array(state).reshape(1, len(state)), y, batch_size=batchSize, nb_epoch=1, verbose=1)\n",
    "            state = new_state\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        else:\n",
    "            #Experience replay storage\n",
    "            if (len(replay) < buffer): #if buffer not filled, add to it\n",
    "                replay.append((state, action, reward, new_state))\n",
    "            else: #if buffer full, overwrite old values\n",
    "                if (h < (buffer-1)):\n",
    "                    h += 1\n",
    "                else:\n",
    "                    h = 0\n",
    "                replay[h] = (state, action, reward, new_state)\n",
    "                #randomly sample our experience replay memory\n",
    "            \n",
    "            if(len(replay)>batchSize):\n",
    "                minibatch = random.sample(replay, batchSize)\n",
    "                X_train = []\n",
    "                y_train = []\n",
    "                for memory in minibatch:\n",
    "                    #Get max_Q(S',a)\n",
    "                    old_state, action, reward, new_state = memory\n",
    "                    old_qval = model.predict(np.array(old_state).reshape(1,len(old_state)), batch_size=1)\n",
    "                    newQ = model.predict(np.array(new_state).reshape(1,len(new_state)), batch_size=1)\n",
    "                    maxQ = np.max(newQ)\n",
    "                    y = old_qval[:]\n",
    "                    if reward != 0: #non-terminal state\n",
    "                        update = (reward + (gamma * maxQ))\n",
    "                    else: #terminal state\n",
    "                        update = reward\n",
    "                    y[0][action] = update\n",
    "                    X_train.append(np.array(old_state).reshape(len(old_state),))\n",
    "                    y_train.append(np.array(y).reshape(9,))\n",
    "    \n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "                print(\"Game #: %s\" % (i,))\n",
    "                model.fit(X_train, y_train, batch_size=batchSize, nb_epoch=1, verbose=1)\n",
    "                state = new_state\n",
    "            clear_output(wait=True)\n",
    "        # update exploitation / exploration strategy\n",
    "        if epsilon > 0.1:\n",
    "            epsilon -= (1.0/epochs)\n",
    "    \n",
    "        # save the model every 1000 epochs\n",
    "        if i%1000 == 0:\n",
    "            model.save(\"model_dql_oxo_dense.dqf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
